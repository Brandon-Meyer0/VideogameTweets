{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Sentiment Analysis - TEST script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk import FreqDist\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import itertools as it\n",
    "import emoji\n",
    "import re\n",
    "import spacy\n",
    "import fileinput\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "twitter_training = pd.read_csv('twitter_training.csv', sep=',', names=['Tweet ID','Entity','Sentiment','Tweet_content'])\n",
    "twitter_validation = pd.read_csv('twitter_validation.csv', sep=',', names=['Tweet ID','Entity','Sentiment','Tweet_content'])\n",
    "\n",
    "twitter_training[\"Tweet ID\"] = range(1, len(twitter_training) + 1)\n",
    "twitter_validation[\"Tweet ID\"] = range(1, len(twitter_validation) + 1)\n",
    "\n",
    "# remove dupicate and nan values\n",
    "twitter_training.dropna(inplace=True)\n",
    "twitter_training.drop_duplicates(inplace=True)\n",
    "\n",
    "#remove urls & special characters\n",
    "def remove_urls(text):\n",
    "    \"\"\"Berilgan matndan URL larini o'chiradi\"\"\"\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "def remove_emojis(text):\n",
    "    \"\"\"Berilgan matndan emojilarni o'chiradi\"\"\"\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emojilar\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # simvollar va diagrammalar\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport va turli joylar\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # davlat bayroqlari\n",
    "                               u\"\\U00002702-\\U000027B0\"  # dingbats\n",
    "                               u\"\\U000024C2-\\U0001F251\"  # alamatchilik belgilari\n",
    "                               u\"\\U0001f300-\\U0001f650\"  \n",
    "                               u\"\\u2000-\\u3000\" \n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "twitter_training['Tweet_content'] = twitter_training['Tweet_content'].apply(lambda x: remove_emojis(x))\n",
    "twitter_training['Tweet_content'] = twitter_training['Tweet_content'].apply(lambda x: remove_urls(x))\n",
    "\n",
    "twitter_validation['Tweet_content'] = twitter_validation['Tweet_content'].apply(lambda x: remove_emojis(x))\n",
    "twitter_validation['Tweet_content'] = twitter_validation['Tweet_content'].apply(lambda x: remove_urls(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity Recognition\n",
    "Entities_training = set(twitter_training['Entity'])\n",
    "Entities_validation = set(twitter_validation['Entity'])\n",
    "Entities_training = list(Entities_trainig)\n",
    "Entities_validation = list(Entities_valiadtion)\n",
    "#EntitiesLowered = [item.lower() for item in Entities_t]\n",
    "\n",
    "#See if entiites in both datasets are the same:\n",
    "print(Entities_t)\n",
    "print(len(Entities_t))\n",
    "#print(Entities_v)\n",
    "#print(len(Entities_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = \"on Borderlands 1 I will murder all on Xbox the console of microsoft, is like RDR or Assassin's-creed...\"\n",
    "#comment = \"I love assassinscreedvalhalla\"\n",
    "#comment = twitter_training[\"Tweet_content\"][292]\n",
    "\n",
    "comment = re.sub(r\"[^a-zA-Z4-5 ]\", \"\", comment) #Remove apostrophes, comas, ... \n",
    "print(comment)\n",
    "\n",
    "comment = ' '.join([word for word in comment.split() if word not in numbers_list]) #Remove whitespaces\n",
    "print(comment.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft ORG\n"
     ]
    }
   ],
   "source": [
    "# Entity Recognition\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sentence1= \"Apple is looking at buying U.K. startup for $1 billion so apple\"\n",
    "sentence2= 'on Borderlands 1 i will murder you all on xbox the console of Microsoft, borderlands 2 is like Red Dead Redemption'\n",
    "sentence3= \"in Overwatch you can play as hunter, in Overwatch you can play as a outlaw\"\n",
    "doc1 = nlp(sentence2.title())\n",
    "#doc2 = nlp(twitter_training[\"Tweet_content\"][292])\n",
    "for ent in doc1.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xbox(SeriesX)\n",
      "Yes! xbox(xseries) is containing Xbox.\n"
     ]
    }
   ],
   "source": [
    "Entities = set(twitter_training['Entity'])\n",
    "Entities = list(Entities)\n",
    "EntitiesLowered = [item.lower() for item in Entities]\n",
    "word=\"Xbox\"\n",
    "print('Xbox(SeriesX)'.replace(\" \", \"\"))\n",
    "for i in EntitiesLowered:\n",
    "    if word.lower() in i:\n",
    "        print(f\"Yes! {i} is containing {word}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Want to have a list of all entities\n",
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    entity_tokens=[]\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in (\"ORG\", \"PRODUCT\", \"WORK_OF_ART\"):\n",
    "            entity_tokens.append(ent.text)\n",
    "    return \" \".join(entity_tokens)\n",
    "entity_tokens = twitter_training['Tweet_content'].apply(lambda x: preprocess(x))\n",
    "print(entity_tokens)\n",
    "#No longer usable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word=\"I love Borderlands\"\n",
    "#word=\"ps5\"\n",
    "for i in Entities:\n",
    "    if re.search(word.replace(\" \", \"\"), i, re.IGNORECASE):\n",
    "        print(f\"Yes! {i} is containing {word}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found one of em\n"
     ]
    }
   ],
   "source": [
    "keyword_list = ['motorcycle', 'bike', 'cycle', 'dirtbike']\n",
    "all_text = \"what kind of bike do you like?\"\n",
    "for item in keyword_list:\n",
    "      if item in all_text:\n",
    "            print ('found one of em')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red Dead Redemption\n"
     ]
    }
   ],
   "source": [
    "def find_word(text, search):\n",
    "\n",
    "   result = re.findall('\\\\b'+search+'\\\\b', text, flags=re.IGNORECASE)\n",
    "   if len(result)>0:\n",
    "      print(text)\n",
    "   else:\n",
    "      print(False)\n",
    "\n",
    "find_word(\"Red Dead Redemption\", \"red dead redemption\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2= 'on Borderlands 1 i will murder you all on Xbox the console of Microsoft, Borderlands 2 is like Read Dead Redemption'\n",
    "def replace_if_not_in_keeplist(match):\n",
    "    word = match.group()\n",
    "    if re.search(word.replace(\" \", \"\"), i, re.IGNORECASE):\n",
    "        return word\n",
    "    return word.lower()\n",
    "\n",
    "s2 = re.sub(r\"\\w+\", replace_if_not_in_keeplist, sentence2)\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "stop_words = stopwords.words('english')\n",
    "stopwords_dict = Counter(stop_words)\n",
    "#text = ' '.join([word for word in text.split() if word not in stopwords_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('pattern 1', 'replacement text 1'), ('pattern 2', 'replacement text 2')])\n"
     ]
    }
   ],
   "source": [
    "#Trying to replace entiites in text (7/26/2023)\n",
    "import fileinput\n",
    "\n",
    "text = \"\"\"Want to replacement text 1 = pattern 1\"\"\"\n",
    "fields = {\"pattern 1\": \"replacement text 1\", \"pattern 2\": \"replacement text 2\"}\n",
    "\n",
    "print(fields.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Want to replacement text 1 = replacement text 1\n"
     ]
    }
   ],
   "source": [
    "words_to_replace = r'\\bpattern \\d+\\b'\n",
    "\n",
    "def replace_words_using_dict(matchobj):\n",
    "    key = matchobj.group(0)\n",
    "    return fields.get(key, key)\n",
    "\n",
    "print(re.sub(words_to_replace, replace_words_using_dict, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love RedDeadRedemption\n"
     ]
    }
   ],
   "source": [
    "#This code successfully worked\n",
    "\n",
    "strValue = \"\"\"I love rdr\"\"\"\n",
    "# Dictionary containing mapping of \n",
    "# values to be replaced and replacement values\n",
    "dictOfStrings = {\"RedDeadRedemption\":  \"rdr\", \"Xbox\": \"xbox\"}\n",
    "\n",
    "# Iterate over all key-value pairs in dict and \n",
    "# replace each key by the value in the string\n",
    "\n",
    "for dict_key, dict_value in dictOfStrings.items():\n",
    "    strValue = strValue.replace(dict_value, dict_key)\n",
    "print(strValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_dict(values, searchFor):\n",
    "    for k in values:\n",
    "        for v in values[k]:\n",
    "            if searchFor in v:\n",
    "                return k\n",
    "    return None\n",
    "#print(search_dict(entity_dict, 'rdr'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'comment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-2e3688598e2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Entity recognition with function nlp (7/23/2023)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdoc1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0ment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"ORG\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"PRODUCT\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"WORK_OF_ART\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'comment' is not defined"
     ]
    }
   ],
   "source": [
    "# Entity recognition with function nlp (7/23/2023)\n",
    "doc1 = nlp(comment)\n",
    "for ent in doc1.ents:\n",
    "    if ent.label_ in (\"ORG\", \"PRODUCT\", \"WORK_OF_ART\"):\n",
    "        print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Stopword list\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words += list(string.punctuation)\n",
    "stop_words += ['....','...', '..', '‚Äô', \"''\", '``', '-', \"'\", \"([a-zA-Z]+(?:'[a-z]+)?)\"]\n",
    "stopwords_dict = Counter(stop_words)\n",
    "#text_tokens = word_tokenize(sen)\n",
    "#tokens_without_sw = [word for word in text_tokens if not word in stopwords_dict]\n",
    "comment = ' '.join([word for word in comment.split() if word not in stop_words])\n",
    "print(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find Entities in Sentence with our list of Entities\n",
    "def Entity_replace(sentence):\n",
    "    #numbers_list = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    sentence = re.sub(r\"[^a-zA-Z0-9] \", \"\", sentence)\n",
    "    sentence = ' '.join([word for word in sentence.split() if word not in numbers_list])\n",
    "    doc1 = nlp(sentence)\n",
    "    for ent in doc1.ents:\n",
    "        if ent.label_ in (\"ORG\", \"PRODUCT\", \"WORK_OF_ART\"):\n",
    "            #print(ent.text, ent.label_)\n",
    "            for item in EntitiesLowered:\n",
    "                if ent.text.lower().replace(\" \", \"\") in item:\n",
    "                    #print(\"I found! \" + ent.text.lower().replace(\" \", \"\") +  \" and will raplace with \" + item)\n",
    "                    print(ent.text.lower().replace(\" \", \"\"), item)\n",
    "                    sentence = sentence.replace(ent.text, item)\n",
    "    return sentence.lower()\n",
    "\n",
    "print(Entity_replace(comment))\n",
    "#print(sen)\n",
    "#print(sen.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Emoji Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶ñ\n"
     ]
    }
   ],
   "source": [
    "from emoji import emojize\n",
    "print(emojize(':T-Rex:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractEmojis(text): \n",
    "  RegExp re = RegExp(r'[\\p{Extended_Pictographic}\\u{1F3FB}-\\u{1F3FF}\\u{1F9B0}-\\u{1F9B3}]', unicode: true)\n",
    "  return re.allMatches(text).map((z) => z.group(0)).toList().join(\"\")\n",
    "\n",
    "\n",
    "print(extractEmojis(\"Hey everyone ü•∫ü•∫ü•∫\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_count(text):\n",
    "\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emoji_list.append(word)\n",
    "    \n",
    "    return emoji_list\n",
    "\n",
    "line = [\"ü§î üôà me as√≠, se üòå ds üíïüë≠üëô hello üë©üèæ‚Äçüéì emoji hello üë®‚Äçüë©‚Äçüë¶‚Äçüë¶ how are üòä you todayüôÖüèΩüôÖüèΩ\"]\n",
    "\n",
    "counter = split_count(line[0])\n",
    "print(' '.join(emoji for emoji in counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'üçã': 1, 'üòÅ': 1, '‚≠ê': 1})\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'phrases' : [\"Smiley emoticon rocks!üçã I like you.\\U0001f601\", \n",
    "                                \"Catch a falling ‚≠êÔ∏è and put it in your pocket\"]})\n",
    "emoji_count = defaultdict(int)\n",
    "for i in df['phrases']:\n",
    "    for emoji in re.findall(u'[\\U0001f300-\\U0001f650]|[\\u2000-\\u3000]', i):\n",
    "        emoji_count[emoji] += 1\n",
    "\n",
    "print (emoji_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unicode('‚≠êÔ∏è ', 'utf-8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Replacing names of games by entity dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace names of games by entity list\n",
    "\n",
    "example='I love cod black ops'\n",
    "\n",
    "def find_entities_list(text):\n",
    "    text = re.sub(r\"[^a-zA-Z ]\", \"\", text)\n",
    "    text = ' '.join([word for word in text.split()])\n",
    "    text = text.lower()\n",
    "    for entity_names in entity_dict:\n",
    "        for game_names in entity_dict[entity_names]:\n",
    "            if game_names.lower() in text:\n",
    "                #print(game_names)\n",
    "                text = text.replace(game_names, entity_names)\n",
    "                #print(text)\n",
    "    return text\n",
    "\n",
    "print(find_entities_list(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_entities_list(twitter_training[\"Tweet_content\"][24432]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_training['Tweet_content'] = [find_entities_list(word) for word in twitter_training['Tweet_content']]\n",
    "twitter_validation['Tweet_content'] = [find_entities_list(word) for word in twitter_validation['Tweet_content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twitter_validation['Tweet_content'])\n",
    "print(twitter_training['Tweet_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
