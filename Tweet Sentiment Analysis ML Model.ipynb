{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6cd5ba1",
   "metadata": {},
   "source": [
    "# Tweet Sentiment Analysis\n",
    "\n",
    "This project uses Machine Learning models to classify tweets into a labelled sentiment using the Twitter Sentiment Analysis dataset from Kaggle (https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis?resource=download) which focuses on Entity-level sentiment analysis on multi-lingual tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59398f65",
   "metadata": {},
   "source": [
    "### Importing libraries\n",
    "\n",
    "First we import the libraries we will need in the project and check the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8aa2e217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import string\n",
    "import itertools as it\n",
    "import emoji\n",
    "import fileinput\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix,roc_auc_score,classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import load_model\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a4ad5f",
   "metadata": {},
   "source": [
    "The NLTK resources will be downloaded (which is why this cell should only be run *once*) and used to tokenize the words in each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7609ba5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bcancinomeyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bcancinomeyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Download resources\n",
    "#pre-trained sentence tokenizer\n",
    "nltk.download('punkt')\n",
    "#list of stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fbbd01",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "We begin by looking at our data so we know what we have to work with and what we will cleanup. We begin our EDA by opening both of our datasets as Pandas Dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4730e8f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('twitter_training.csv', sep=',', names=[\"Tweet ID\", \"Entity\", \"Sentiment\", \"Tweet Content\"])\n",
    "df.head()\n",
    "valid_df = pd.read_csv('twitter_validation.csv', sep=',', names=[\"Tweet ID\", \"Entity\", \"Sentiment\", \"Tweet Content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9ee3e8f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Tweet ID       Entity Sentiment  \\\n",
      "0         1  Borderlands  Positive   \n",
      "1         2  Borderlands  Positive   \n",
      "2         3  Borderlands  Positive   \n",
      "3         4  Borderlands  Positive   \n",
      "4         5  Borderlands  Positive   \n",
      "\n",
      "                                       Tweet Content  \n",
      "0  im getting on borderlands and i will murder yo...  \n",
      "1  i am coming to the borders and i will kill you...  \n",
      "2  im getting on borderlands and i will kill you ...  \n",
      "3  im coming on borderlands and i will murder you...  \n",
      "4  im getting on borderlands 2 and i will murder ...      Tweet ID     Entity Sentiment  \\\n",
      "0         1   Facebook   Neutral   \n",
      "1         2     Amazon   Neutral   \n",
      "2         3  Microsoft  Negative   \n",
      "3         4      CS-GO  Negative   \n",
      "4         5     Google   Neutral   \n",
      "\n",
      "                                       Tweet Content  \n",
      "0  i mentioned on facebook that i was struggling ...  \n",
      "1  bbc news - amazon boss jeff bezos rejects clai...  \n",
      "2  @microsoft why do i pay for word when it funct...  \n",
      "3  csgo matchmaking is so full of closet hacking,...  \n",
      "4  now the president is slapping americans in the...  \n"
     ]
    }
   ],
   "source": [
    "df[\"Tweet ID\"] = range(1, len(df) + 1)\n",
    "valid_df[\"Tweet ID\"] = range(1, len(valid_df) + 1)\n",
    "print(df.head(), valid_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5f5d3fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'im getting on borderlands and i will murder you all ,'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Tweet Content'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcbb523",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "\n",
    "We begin our cleaning by dropping any duplicate and NAN values present in both dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1edf2c12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "valid_df.dropna(inplace=True)\n",
    "valid_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24eb308d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Positive', 'Negative', 'Neutral'}\n"
     ]
    }
   ],
   "source": [
    "df['Sentiment'].replace(to_replace='Irrelevant', value='Neutral', inplace=True)\n",
    "valid_df['Sentiment'].replace(to_replace='Irrelevant', value='Neutral', inplace=True)\n",
    "print(set(df['Sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ad7c279f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet ID</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>i mentioned on facebook that i was struggling ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>bbc news - amazon boss jeff bezos rejects clai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@microsoft why do i pay for word when it funct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>CS-GO</td>\n",
       "      <td>Negative</td>\n",
       "      <td>csgo matchmaking is so full of closet hacking,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Google</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>now the president is slapping americans in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>996</td>\n",
       "      <td>GrandTheftAuto(GTA)</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>toronto is the arts and culture capital of ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>997</td>\n",
       "      <td>CS-GO</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>this is actually a good move tot bring more vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>998</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>today sucked so its time to drink wine n play ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>999</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Positive</td>\n",
       "      <td>bought a fraction of microsoft today. small wins.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1000</td>\n",
       "      <td>johnson&amp;johnson</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>johnson &amp; johnson to stop selling talc baby po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Tweet ID               Entity Sentiment  \\\n",
       "0           1             Facebook   Neutral   \n",
       "1           2               Amazon   Neutral   \n",
       "2           3            Microsoft  Negative   \n",
       "3           4                CS-GO  Negative   \n",
       "4           5               Google   Neutral   \n",
       "..        ...                  ...       ...   \n",
       "995       996  GrandTheftAuto(GTA)   Neutral   \n",
       "996       997                CS-GO   Neutral   \n",
       "997       998          Borderlands  Positive   \n",
       "998       999            Microsoft  Positive   \n",
       "999      1000      johnson&johnson   Neutral   \n",
       "\n",
       "                                         Tweet Content  \n",
       "0    i mentioned on facebook that i was struggling ...  \n",
       "1    bbc news - amazon boss jeff bezos rejects clai...  \n",
       "2    @microsoft why do i pay for word when it funct...  \n",
       "3    csgo matchmaking is so full of closet hacking,...  \n",
       "4    now the president is slapping americans in the...  \n",
       "..                                                 ...  \n",
       "995   toronto is the arts and culture capital of ca...  \n",
       "996  this is actually a good move tot bring more vi...  \n",
       "997  today sucked so its time to drink wine n play ...  \n",
       "998  bought a fraction of microsoft today. small wins.  \n",
       "999  johnson & johnson to stop selling talc baby po...  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_strings_mask = pd.to_numeric(df['Tweet Content'], errors='coerce').isna()\n",
    "df.loc[non_strings_mask]\n",
    "non_strings_mask = pd.to_numeric(valid_df['Tweet Content'], errors='coerce').isna()\n",
    "valid_df.loc[non_strings_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3d82ea25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Tweet ID  Entity Sentiment  \\\n",
      "74677     73992  Nvidia  Positive   \n",
      "74678     73993  Nvidia  Positive   \n",
      "74679     73994  Nvidia  Positive   \n",
      "74680     73995  Nvidia  Positive   \n",
      "74681     73996  Nvidia  Positive   \n",
      "\n",
      "                                           Tweet Content  \n",
      "74677  just realized that the windows partition of my...  \n",
      "74678  just realized that my mac window partition is ...  \n",
      "74679  just realized the windows partition of my mac ...  \n",
      "74680  just realized between the windows partition of...  \n",
      "74681  just like the windows partition of my mac is l...  \n",
      "     Tweet ID               Entity Sentiment  \\\n",
      "995       996  GrandTheftAuto(GTA)   Neutral   \n",
      "996       997                CS-GO   Neutral   \n",
      "997       998          Borderlands  Positive   \n",
      "998       999            Microsoft  Positive   \n",
      "999      1000      johnson&johnson   Neutral   \n",
      "\n",
      "                                         Tweet Content  \n",
      "995   toronto is the arts and culture capital of ca...  \n",
      "996  this is actually a good move tot bring more vi...  \n",
      "997  today sucked so its time to drink wine n play ...  \n",
      "998  bought a fraction of microsoft today. small wins.  \n",
      "999  johnson & johnson to stop selling talc baby po...  \n"
     ]
    }
   ],
   "source": [
    "#remove urls & special characters\n",
    "def remove_urls(text):\n",
    "    \"\"\"Berilgan matndan URL larini o'chiradi\"\"\"\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "def remove_emojis(text):\n",
    "    \"\"\"Berilgan matndan emojilarni o'chiradi\"\"\"\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emojilar\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # simvollar va diagrammalar\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport va turli joylar\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # davlat bayroqlari\n",
    "                               u\"\\U00002702-\\U000027B0\"  # dingbats\n",
    "                               u\"\\U000024C2-\\U0001F251\"  # alamatchilik belgilari\n",
    "                               u\"\\U0001f300-\\U0001f650\"  \n",
    "                               u\"\\u2000-\\u3000\" \n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "df['Tweet Content'] = df['Tweet Content'].apply(lambda x: remove_emojis(x))\n",
    "df['Tweet Content'] = df['Tweet Content'].apply(lambda x: remove_urls(x))\n",
    "valid_df['Tweet Content'] = valid_df['Tweet Content'].apply(lambda x: remove_emojis(x))\n",
    "valid_df['Tweet Content'] = valid_df['Tweet Content'].apply(lambda x: remove_urls(x))\n",
    "\n",
    "\n",
    "df['Tweet Content'] = df['Tweet Content'].apply(str.lower)\n",
    "valid_df['Tweet Content'] = valid_df['Tweet Content'].apply(str.lower)\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list += ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "stopwords_list += list(string.punctuation)\n",
    "stopwords_list += ['....','...', '..', '’', \"''\", '``', '-', \"'\", \"([a-zA-Z]+(?:'[a-z]+)?)\"]\n",
    "\n",
    "print(df.tail())\n",
    "print(valid_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "23d6b4a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['just',\n",
       " 'realized',\n",
       " 'that',\n",
       " 'the',\n",
       " 'windows',\n",
       " 'partition',\n",
       " 'of',\n",
       " 'my',\n",
       " 'mac',\n",
       " 'is',\n",
       " 'like',\n",
       " '6',\n",
       " 'years',\n",
       " 'behind',\n",
       " 'nvidia',\n",
       " 'drivers',\n",
       " 'and',\n",
       " 'i',\n",
       " 'have',\n",
       " 'no',\n",
       " 'idea',\n",
       " 'how',\n",
       " 'i',\n",
       " 'did',\n",
       " 'not',\n",
       " 'notice']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(df['Tweet Content'][74677])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a55cbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining entity dictionary\n",
    "entity_dict = { \n",
    "                \"RedDeadRedemption(RDR)\" : ['rdr', 'red dead redemption', 'red dead'], \n",
    "                \"Microsoft\": ['microsoft'],\n",
    "                \"Xbox(XSeries)\":['xbox', 'series x', 'series s', 'xbox one', 'xseries'], \n",
    "                \"AssassinsCreed\": ['assassinscreed', 'assassins creed'], \n",
    "                \"CallOfDutyBlackopsColdWar\": ['black ops', 'cold war', 'callOfdutyblackopscoldWar'],\n",
    "                \"FIFA\": ['fifa'],\n",
    "                \"TomClancysGhostRecon\": ['ghost recon', 'ghostrecon'],\n",
    "                \"Google\": ['google'],\n",
    "                \"PlayStation(PS)\": ['ps5', 'playstation', 'ps4', 'PS'],\n",
    "                \"Facebook\": ['facebook'],\n",
    "                \"GrandTheftAuto(GTA)\": ['gta', 'grand theft auto'],\n",
    "                \"PlayerUnknownsBattlegrounds(PUBG)\": ['pubg', 'player unknowns battlegrounds', 'PlayerUnknownsBattlegrounds'],\n",
    "                \"Hearthstone\": ['hearthstone'],\n",
    "                \"MaddenNFL\": ['madden'],\n",
    "                \"CallOfDuty\": ['modern warfare', 'call of duty', 'cod'],\n",
    "                \"Fortnite\": ['fortnitegame', 'fortnite'],\n",
    "                \"Verizon\": ['verizon'],\n",
    "                \"Nvidia\": ['nvidia'],\n",
    "                \"Amazon\": ['amazon'],\n",
    "                \"WorldOfCraft\": ['wow', 'world of warcraft'],\n",
    "                \"ApexLegends\": ['apex legends', 'apex', 'apexlegends'],\n",
    "                \"CS-GO\": ['csgo', 'counter strike'],\n",
    "                \"johnson&johnson\": ['johnson&johnson', 'johnson & johnson'],\n",
    "                \"HomeDepot\": ['homedepot', 'home depot'],\n",
    "                \"NBA2K\": ['nba'],\n",
    "                \"Overwatch\": ['overwatch'],\n",
    "                \"LeagueOfLegends\": ['lol', 'league of legends'],\n",
    "                \"Borderlands\": ['borderlands'],\n",
    "                \"TomClancysRainbowSix\": ['rainbow six', 'rainbow six siege', 'rainbowsix'],\n",
    "                \"Dota\": ['dota'],\n",
    "                \"Battlefield\": ['battlefield'],\n",
    "                \"Cyberpunk2077\": ['cyberpunkgame', 'cyberpunk2077', 'cyberpunk'],\n",
    "                \"NintendoSwitch\": ['nintendo switch', 'nintendo'],\n",
    "                \"Windows\": ['windows', 'window']\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b74d721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet ID</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet Content</th>\n",
       "      <th>Tokenized Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "      <td>[im, getting, on, borderlands, and, i, will, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>i am coming to the borders and i will kill you...</td>\n",
       "      <td>[i, am, coming, to, the, borders, and, i, will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td>[im, getting, on, borderlands, and, i, will, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>[im, coming, on, borderlands, and, i, will, mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td>[im, getting, on, borderlands, 2, and, i, will...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tweet ID       Entity Sentiment  \\\n",
       "0         1  Borderlands  Positive   \n",
       "1         2  Borderlands  Positive   \n",
       "2         3  Borderlands  Positive   \n",
       "3         4  Borderlands  Positive   \n",
       "4         5  Borderlands  Positive   \n",
       "\n",
       "                                       Tweet Content  \\\n",
       "0  im getting on borderlands and i will murder yo...   \n",
       "1  i am coming to the borders and i will kill you...   \n",
       "2  im getting on borderlands and i will kill you ...   \n",
       "3  im coming on borderlands and i will murder you...   \n",
       "4  im getting on borderlands 2 and i will murder ...   \n",
       "\n",
       "                                      Tokenized Text  \n",
       "0  [im, getting, on, borderlands, and, i, will, m...  \n",
       "1  [i, am, coming, to, the, borders, and, i, will...  \n",
       "2  [im, getting, on, borderlands, and, i, will, k...  \n",
       "3  [im, coming, on, borderlands, and, i, will, mu...  \n",
       "4  [im, getting, on, borderlands, 2, and, i, will...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "df['Tokenized Text'] = df['Tweet Content'].apply(tokenize_text)\n",
    "\n",
    "\n",
    "df['Tokenized Text'] = [word for word in df['Tokenized Text'] if word not in stopwords_list]\n",
    "df['Tokenized Text'] = [word for word in df['Tokenized Text'] if word not in stopwords_list]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3211cde6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['im',\n",
       " 'getting',\n",
       " 'on',\n",
       " 'borderlands',\n",
       " 'and',\n",
       " 'i',\n",
       " 'will',\n",
       " 'murder',\n",
       " 'you',\n",
       " 'all',\n",
       " ',']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Tokenized Text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06bb230",
   "metadata": {},
   "source": [
    "## Creating a Long Short-Term Memory (LTSM) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c5e66a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['Tokenized Text'])\n",
    "sequences = tokenizer.texts_to_sequences(df['Tokenized Text'])\n",
    "X = pad_sequences(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "883c82fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1deae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19b79610",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_sequence_length = X.shape[1]\n",
    "embedding_dim = 100\n",
    "lstm_units = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fce45381",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "model.add(LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "632d2e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad2ee117",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_encoded = np.eye(len(label_encoder.classes_))[y_train]\n",
    "y_test_encoded = np.eye(len(label_encoder.classes_))[y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9fbe179",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "740/740 [==============================] - 464s 624ms/step - loss: 0.7342 - accuracy: 0.6774 - val_loss: 0.5658 - val_accuracy: 0.7676\n",
      "Epoch 2/10\n",
      "740/740 [==============================] - 481s 650ms/step - loss: 0.3581 - accuracy: 0.8624 - val_loss: 0.3990 - val_accuracy: 0.8456\n",
      "Epoch 3/10\n",
      "740/740 [==============================] - 508s 687ms/step - loss: 0.2219 - accuracy: 0.9139 - val_loss: 0.3638 - val_accuracy: 0.8651\n",
      "Epoch 4/10\n",
      "740/740 [==============================] - 492s 665ms/step - loss: 0.1612 - accuracy: 0.9363 - val_loss: 0.3483 - val_accuracy: 0.8742\n",
      "Epoch 5/10\n",
      "740/740 [==============================] - 538s 727ms/step - loss: 0.1386 - accuracy: 0.9451 - val_loss: 0.3925 - val_accuracy: 0.8721\n",
      "Epoch 6/10\n",
      "740/740 [==============================] - 489s 660ms/step - loss: 0.1137 - accuracy: 0.9545 - val_loss: 0.3735 - val_accuracy: 0.8801\n",
      "Epoch 7/10\n",
      "740/740 [==============================] - 488s 659ms/step - loss: 0.1006 - accuracy: 0.9590 - val_loss: 0.3819 - val_accuracy: 0.8758\n",
      "Epoch 8/10\n",
      "740/740 [==============================] - 461s 623ms/step - loss: 0.0897 - accuracy: 0.9632 - val_loss: 0.4034 - val_accuracy: 0.8740\n",
      "Epoch 9/10\n",
      "740/740 [==============================] - 449s 607ms/step - loss: 0.0814 - accuracy: 0.9665 - val_loss: 0.4232 - val_accuracy: 0.8806\n",
      "Epoch 10/10\n",
      "740/740 [==============================] - 508s 686ms/step - loss: 0.0745 - accuracy: 0.9686 - val_loss: 0.4270 - val_accuracy: 0.8865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2210f63b7f0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 10\n",
    "model.fit(X_train, y_train_encoded, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed814652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463/463 [==============================] - 20s 42ms/step - loss: 0.4390 - accuracy: 0.8862\n",
      "Test Loss: 0.4390312433242798, Test Accuracy: 0.8862162232398987\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ca22e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('tweetsentiment_lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "627352fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model(\"tweetsentiment_lstm_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ffe44014",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sequences = tokenizer.texts_to_sequences(valid_df[\"Tweet Content\"])\n",
    "new_X = pad_sequences(new_sequences, maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "86e79c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 36ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = loaded_model.predict(new_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "35a675d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_prob = \n",
    "class_predictions = np.argmax(class_predictions, axis=1)\n",
    "\n",
    "actual_labels = valid_df[\"Sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "864f772b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Singleton array 0 cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9592\\181068780.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactual_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    305\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m     \"\"\"\n\u001b[1;32m--> 307\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not supported\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \"\"\"\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    327\u001b[0m     \"\"\"\n\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m     \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    327\u001b[0m     \"\"\"\n\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m     \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"shape\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m             raise TypeError(\n\u001b[0m\u001b[0;32m    270\u001b[0m                 \u001b[1;34m\"Singleton array %r cannot be considered a valid collection.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m             )\n",
      "\u001b[1;31mTypeError\u001b[0m: Singleton array 0 cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(actual_labels, class_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b5e555",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
