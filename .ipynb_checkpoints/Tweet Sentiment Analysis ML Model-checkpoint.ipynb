{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6cd5ba1",
   "metadata": {},
   "source": [
    "# Tweet Sentiment Analysis\n",
    "\n",
    "This project uses Machine Learning models to classify tweets into a labelled sentiment using the Twitter Sentiment Analysis dataset from Kaggle (https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis?resource=download) which focuses on Entity-level sentiment analysis on multi-lingual tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59398f65",
   "metadata": {},
   "source": [
    "### Importing libraries\n",
    "\n",
    "First we import the libraries we will need in the project and check the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8aa2e217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import string\n",
    "import itertools as it\n",
    "import emoji\n",
    "import fileinput\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix,roc_auc_score,classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a4ad5f",
   "metadata": {},
   "source": [
    "The NLTK resources will be downloaded (which is why this cell should only be run *once*) and used to tokenize the words in each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7609ba5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Download resources\n",
    "#pre-trained sentence tokenizer\n",
    "nltk.download('punkt')\n",
    "#list of stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10613fba",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "We begin by looking at our data so we know what we have to work with and what we will cleanup. We begin our EDA by opening both of our datasets as Pandas Dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4730e8f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('twitter_training.csv', sep=',', names=[\"Tweet ID\", \"Entity\", \"Sentiment\", \"Tweet Content\"])\n",
    "df.head()\n",
    "valid_df = pd.read_csv('twitter_validation.csv', sep=',', names=[\"Tweet ID\", \"Entity\", \"Sentiment\", \"Tweet Content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee3e8f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"Tweet ID\"] = range(1, len(df) + 1)\n",
    "df[\"Tweet ID\"] = range(1, len(valid_df) + 1)\n",
    "print(df.head(), valid_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f5d3fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Tweet Content'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcbb523",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "\n",
    "We begin our cleaning by dropping any duplicate and NAN values present in both dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edf2c12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "valid_df.dropna(inplace=True)\n",
    "valid_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24eb308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentiment'].replace(to_replace='Irrelevant', value='Neutral', inplace=True)\n",
    "valid_df['Sentiment'].replace(to_replace='Irrelevant', value='Neutral', inplace=True)\n",
    "print(set(df['Sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45122642",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "non_strings_mask = pd.to_numeric(df['Tweet Content'], errors='coerce').isna()\n",
    "df.loc[non_strings_mask]\n",
    "non_strings_mask = pd.to_numeric(valid_df['Tweet Content'], errors='coerce').isna()\n",
    "valid_df.loc[non_strings_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d82ea25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet ID</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74677</th>\n",
       "      <td>74678</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>just realized that the windows partition of my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74678</th>\n",
       "      <td>74679</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>just realized that my mac window partition is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74679</th>\n",
       "      <td>74680</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>just realized the windows partition of my mac ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74680</th>\n",
       "      <td>74681</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>just realized between the windows partition of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74681</th>\n",
       "      <td>74682</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>just like the windows partition of my mac is l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Tweet ID  Entity Sentiment  \\\n",
       "74677     74678  Nvidia  Positive   \n",
       "74678     74679  Nvidia  Positive   \n",
       "74679     74680  Nvidia  Positive   \n",
       "74680     74681  Nvidia  Positive   \n",
       "74681     74682  Nvidia  Positive   \n",
       "\n",
       "                                           Tweet Content  \n",
       "74677  just realized that the windows partition of my...  \n",
       "74678  just realized that my mac window partition is ...  \n",
       "74679  just realized the windows partition of my mac ...  \n",
       "74680  just realized between the windows partition of...  \n",
       "74681  just like the windows partition of my mac is l...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove urls & special characters\n",
    "def remove_urls(text):\n",
    "    \"\"\"Berilgan matndan URL larini o'chiradi\"\"\"\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "def remove_emojis(text):\n",
    "    \"\"\"Berilgan matndan emojilarni o'chiradi\"\"\"\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emojilar\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # simvollar va diagrammalar\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport va turli joylar\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # davlat bayroqlari\n",
    "                               u\"\\U00002702-\\U000027B0\"  # dingbats\n",
    "                               u\"\\U000024C2-\\U0001F251\"  # alamatchilik belgilari\n",
    "                               u\"\\U0001f300-\\U0001f650\"  \n",
    "                               u\"\\u2000-\\u3000\" \n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "df['Tweet Content'] = df['Tweet Content'].apply(lambda x: remove_emojis(x))\n",
    "df['Tweet Content'] = df['Tweet Content'].apply(lambda x: remove_urls(x))\n",
    "\n",
    "df['Tweet Content'] = df['Tweet Content'].apply(str.lower)\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list += ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "stopwords_list += list(string.punctuation)\n",
    "stopwords_list += ['....','...', '..', '’', \"''\", '``', '-', \"'\", \"([a-zA-Z]+(?:'[a-z]+)?)\"]\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23d6b4a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Just',\n",
       " 'realized',\n",
       " 'that',\n",
       " 'the',\n",
       " 'Windows',\n",
       " 'partition',\n",
       " 'of',\n",
       " 'my',\n",
       " 'Mac',\n",
       " 'is',\n",
       " 'like',\n",
       " '6',\n",
       " 'years',\n",
       " 'behind',\n",
       " 'Nvidia',\n",
       " 'drivers',\n",
       " 'and',\n",
       " 'I',\n",
       " 'have',\n",
       " 'no',\n",
       " 'idea',\n",
       " 'how',\n",
       " 'I',\n",
       " 'did',\n",
       " 'not',\n",
       " 'notice']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(df['Tweet Content'][74677])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a55cbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining entity dictionary\n",
    "entity_dict = { \n",
    "                \"RedDeadRedemption(RDR)\" : ['rdr', 'red dead redemption', 'red dead'], \n",
    "                \"Microsoft\": ['microsoft'],\n",
    "                \"Xbox(XSeries)\":['xbox', 'series x', 'series s', 'xbox one', 'xseries'], \n",
    "                \"AssassinsCreed\": ['assassinscreed', 'assassins creed'], \n",
    "                \"CallOfDutyBlackopsColdWar\": ['black ops', 'cold war', 'callOfdutyblackopscoldWar'],\n",
    "                \"FIFA\": ['fifa'],\n",
    "                \"TomClancysGhostRecon\": ['ghost recon', 'ghostrecon'],\n",
    "                \"Google\": ['google'],\n",
    "                \"PlayStation(PS)\": ['ps5', 'playstation', 'ps4', 'PS'],\n",
    "                \"Facebook\": ['facebook'],\n",
    "                \"GrandTheftAuto(GTA)\": ['gta', 'grand theft auto'],\n",
    "                \"PlayerUnknownsBattlegrounds(PUBG)\": ['pubg', 'player unknowns battlegrounds', 'PlayerUnknownsBattlegrounds'],\n",
    "                \"Hearthstone\": ['hearthstone'],\n",
    "                \"MaddenNFL\": ['madden'],\n",
    "                \"CallOfDuty\": ['modern warfare', 'call of duty', 'cod'],\n",
    "                \"Fortnite\": ['fortnitegame', 'fortnite'],\n",
    "                \"Verizon\": ['verizon'],\n",
    "                \"Nvidia\": ['nvidia'],\n",
    "                \"Amazon\": ['amazon'],\n",
    "                \"WorldOfCraft\": ['wow', 'world of warcraft'],\n",
    "                \"ApexLegends\": ['apex legends', 'apex', 'apexlegends'],\n",
    "                \"CS-GO\": ['csgo', 'counter strike'],\n",
    "                \"johnson&johnson\": ['johnson&johnson', 'johnson & johnson'],\n",
    "                \"HomeDepot\": ['homedepot', 'home depot'],\n",
    "                \"NBA2K\": ['nba'],\n",
    "                \"Overwatch\": ['overwatch'],\n",
    "                \"LeagueOfLegends\": ['lol', 'league of legends'],\n",
    "                \"Borderlands\": ['borderlands'],\n",
    "                \"TomClancysRainbowSix\": ['rainbow six', 'rainbow six siege', 'rainbowsix'],\n",
    "                \"Dota\": ['dota'],\n",
    "                \"Battlefield\": ['battlefield'],\n",
    "                \"Cyberpunk2077\": ['cyberpunkgame', 'cyberpunk2077', 'cyberpunk'],\n",
    "                \"NintendoSwitch\": ['nintendo switch', 'nintendo'],\n",
    "                \"Windows\": ['windows', 'window']\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b74d721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet ID</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet Content</th>\n",
       "      <th>Tokenized Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "      <td>[im, getting, on, borderlands, and, i, will, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>i am coming to the borders and i will kill you...</td>\n",
       "      <td>[i, am, coming, to, the, borders, and, i, will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td>[im, getting, on, borderlands, and, i, will, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>[im, coming, on, borderlands, and, i, will, mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td>[im, getting, on, borderlands, 2, and, i, will...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tweet ID       Entity Sentiment  \\\n",
       "0         1  Borderlands  Positive   \n",
       "1         2  Borderlands  Positive   \n",
       "2         3  Borderlands  Positive   \n",
       "3         4  Borderlands  Positive   \n",
       "4         5  Borderlands  Positive   \n",
       "\n",
       "                                       Tweet Content  \\\n",
       "0  im getting on borderlands and i will murder yo...   \n",
       "1  i am coming to the borders and i will kill you...   \n",
       "2  im getting on borderlands and i will kill you ...   \n",
       "3  im coming on borderlands and i will murder you...   \n",
       "4  im getting on borderlands 2 and i will murder ...   \n",
       "\n",
       "                                      Tokenized Text  \n",
       "0  [im, getting, on, borderlands, and, i, will, m...  \n",
       "1  [i, am, coming, to, the, borders, and, i, will...  \n",
       "2  [im, getting, on, borderlands, and, i, will, k...  \n",
       "3  [im, coming, on, borderlands, and, i, will, mu...  \n",
       "4  [im, getting, on, borderlands, 2, and, i, will...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "df['Tokenized Text'] = df['Tweet Content'].apply(tokenize_text)\n",
    "\n",
    "\n",
    "df['Tokenized Text'] = [word for word in df['Tokenized Text'] if word not in stopwords_list]\n",
    "df['Tokenized Text'] = [word for word in df['Tokenized Text'] if word not in stopwords_list]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3211cde6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['im',\n",
       " 'getting',\n",
       " 'on',\n",
       " 'borderlands',\n",
       " 'and',\n",
       " 'i',\n",
       " 'will',\n",
       " 'murder',\n",
       " 'you',\n",
       " 'all',\n",
       " ',']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Tokenized Text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ce93f4",
   "metadata": {},
   "source": [
    "## Creating a Long Short-Term Memory (LTSM) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c5e66a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['Tokenized Text'])\n",
    "sequences = tokenizer.texts_to_sequences(df['Tokenized Text'])\n",
    "X = pad_sequences(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e71f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82fda7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50a58343",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_sequence_length = X.shape[1]\n",
    "embedding_dim = 100\n",
    "lstm_units = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8141a0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "model.add(LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63adfaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98096350",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_encoded = np.eye(len(label_encoder.classes_))[y_train]\n",
    "y_test_encoded = np.eye(len(label_encoder.classes_))[y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "330b794e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "740/740 [==============================] - 464s 624ms/step - loss: 0.7342 - accuracy: 0.6774 - val_loss: 0.5658 - val_accuracy: 0.7676\n",
      "Epoch 2/10\n",
      "740/740 [==============================] - 481s 650ms/step - loss: 0.3581 - accuracy: 0.8624 - val_loss: 0.3990 - val_accuracy: 0.8456\n",
      "Epoch 3/10\n",
      "740/740 [==============================] - 508s 687ms/step - loss: 0.2219 - accuracy: 0.9139 - val_loss: 0.3638 - val_accuracy: 0.8651\n",
      "Epoch 4/10\n",
      "740/740 [==============================] - 492s 665ms/step - loss: 0.1612 - accuracy: 0.9363 - val_loss: 0.3483 - val_accuracy: 0.8742\n",
      "Epoch 5/10\n",
      "740/740 [==============================] - 538s 727ms/step - loss: 0.1386 - accuracy: 0.9451 - val_loss: 0.3925 - val_accuracy: 0.8721\n",
      "Epoch 6/10\n",
      "740/740 [==============================] - 489s 660ms/step - loss: 0.1137 - accuracy: 0.9545 - val_loss: 0.3735 - val_accuracy: 0.8801\n",
      "Epoch 7/10\n",
      "740/740 [==============================] - 488s 659ms/step - loss: 0.1006 - accuracy: 0.9590 - val_loss: 0.3819 - val_accuracy: 0.8758\n",
      "Epoch 8/10\n",
      "740/740 [==============================] - 461s 623ms/step - loss: 0.0897 - accuracy: 0.9632 - val_loss: 0.4034 - val_accuracy: 0.8740\n",
      "Epoch 9/10\n",
      "740/740 [==============================] - 449s 607ms/step - loss: 0.0814 - accuracy: 0.9665 - val_loss: 0.4232 - val_accuracy: 0.8806\n",
      "Epoch 10/10\n",
      "740/740 [==============================] - 508s 686ms/step - loss: 0.0745 - accuracy: 0.9686 - val_loss: 0.4270 - val_accuracy: 0.8865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2210f63b7f0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 10\n",
    "model.fit(X_train, y_train_encoded, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ce8e8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463/463 [==============================] - 20s 42ms/step - loss: 0.4390 - accuracy: 0.8862\n",
      "Test Loss: 0.4390312433242798, Test Accuracy: 0.8862162232398987\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "027e48c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('tweetsentiment_lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87e994b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model(\"tweetsentiment_lstm_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2f6e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sequences = tokenizer.texts_to_sequences(new_data)\n",
    "new_X = pad_sequences(new_sequences, maxlen=max_sequence_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
