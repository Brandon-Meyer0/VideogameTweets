{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis: EDA \n",
    "### Importing Libraries - Load Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk import FreqDist\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import itertools as it\n",
    "import emoji\n",
    "import re\n",
    "import spacy\n",
    "import fileinput\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_training = pd.read_csv('twitter_training.csv', sep=',', names=['Tweet ID','Entity','Sentiment','Tweet_content'])\n",
    "twitter_validation = pd.read_csv('twitter_validation.csv', sep=',', names=['Tweet ID','Entity','Sentiment','Tweet_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_training[\"Tweet ID\"] = range(1, len(twitter_training) + 1)\n",
    "twitter_validation[\"Tweet ID\"] = range(1, len(twitter_validation) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Exploration of each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_training.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_training.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_validation.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_validation.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove dupicate and nan values\n",
    "twitter_training.dropna(inplace=True)\n",
    "twitter_training.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_training.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_validation.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find emojis in whole dataframe\n",
    "\n",
    "# Dictionary storing emoji counts \n",
    "emoji_count = defaultdict(int)\n",
    "for i in twitter_training['Tweet_content']:\n",
    "    for emoji in re.findall(u'[\\U0001f300-\\U0001f650]|[\\u2000-\\u3000]', i):\n",
    "        emoji_count[emoji] += 1\n",
    "    \n",
    "#By adding this we find more \"emojis\" - is there another way to find these emojis... what if we didn't find them all?\n",
    "#|[\\U0001F600-\\U0001F64F]|[\\U0001F300-\\U0001F5FF]|[\\U0001F680-\\U0001F6FF]|[\\U0001F1E0-\\U0001F1FF]|[\\U00002702-\\U000027B0]|[\\U000024C2-\\U0001F251]\n",
    "print(emoji_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_count = defaultdict(int)\n",
    "for i in twitter_validation['Tweet_content']:\n",
    "    for emoji in re.findall(u'[\\U0001f300-\\U0001f650]|[\\u2000-\\u3000]', i):\n",
    "        emoji_count[emoji] += 1\n",
    "    \n",
    "#By adding this we find more \"emojis\" - is there another way to find these emojis... what if we didn't find them all?\n",
    "#|[\\U0001F600-\\U0001F64F]|[\\U0001F300-\\U0001F5FF]|[\\U0001F680-\\U0001F6FF]|[\\U0001F1E0-\\U0001F1FF]|[\\U00002702-\\U000027B0]|[\\U000024C2-\\U0001F251]\n",
    "print(emoji_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_count = defaultdict(int)\n",
    "for i in twitter_validation['Tweet_content']:\n",
    "    for emoji in re.findall(u'[\\U0001f300-\\U0001f650]|[\\u2000-\\u3000]|[\\U0001F600-\\U0001F64F]|[\\U0001F300-\\U0001F5FF]|[\\U0001F680-\\U0001F6FF]|[\\U0001F1E0-\\U0001F1FF]|[\\U00002702-\\U000027B0]|[\\U000024C2-\\U0001F251]', i):\n",
    "        emoji_count[emoji] += 1\n",
    "    \n",
    "#By adding this we find more \"emojis\" - is there another way to find these emojis... what if we didn't find them all?\n",
    "#|[\\U0001F600-\\U0001F64F]|[\\U0001F300-\\U0001F5FF]|[\\U0001F680-\\U0001F6FF]|[\\U0001F1E0-\\U0001F1FF]|[\\U00002702-\\U000027B0]|[\\U000024C2-\\U0001F251]\n",
    "print(emoji_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove urls & special characters\n",
    "def remove_urls(text):\n",
    "    \"\"\"Berilgan matndan URL larini o'chiradi\"\"\"\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "def remove_emojis(text):\n",
    "    \"\"\"Berilgan matndan emojilarni o'chiradi\"\"\"\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emojilar\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # simvollar va diagrammalar\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport va turli joylar\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # davlat bayroqlari\n",
    "                               u\"\\U00002702-\\U000027B0\"  # dingbats\n",
    "                               u\"\\U000024C2-\\U0001F251\"  # alamatchilik belgilari\n",
    "                               u\"\\U0001f300-\\U0001f650\"  \n",
    "                               u\"\\u2000-\\u3000\" \n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "#def remove_and,or,a,the..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_training['Tweet_content'] = twitter_training['Tweet_content'].apply(lambda x: remove_emojis(x))\n",
    "twitter_training['Tweet_content'] = twitter_training['Tweet_content'].apply(lambda x: remove_urls(x))\n",
    "\n",
    "twitter_validation['Tweet_content'] = twitter_validation['Tweet_content'].apply(lambda x: remove_emojis(x))\n",
    "twitter_validation['Tweet_content'] = twitter_validation['Tweet_content'].apply(lambda x: remove_urls(x))\n",
    "\n",
    "twitter_training['text_lens']=twitter_training['Tweet_content'].apply(lambda x: len(x))\n",
    "twitter_validation['text_lens']=twitter_validation['Tweet_content'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing we indeed removed the emojis:\n",
    "\n",
    "emoji_count = defaultdict(int)\n",
    "for i in twitter_training['Tweet_content']:\n",
    "    for emoji in re.findall(u'[\\U0001f300-\\U0001f650]|[\\u2000-\\u3000]', i):\n",
    "        emoji_count[emoji] += 1\n",
    "\n",
    "print(emoji_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_count = defaultdict(int)\n",
    "for i in twitter_validation['Tweet_content']:\n",
    "    for emoji in re.findall(u'[\\U0001f300-\\U0001f650]|[\\u2000-\\u3000]', i):\n",
    "        emoji_count[emoji] += 1\n",
    "\n",
    "print(emoji_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count information per category\n",
    "data1=twitter_training.groupby(by=[\"Entity\",\"Sentiment\"]).count().reset_index()\n",
    "#data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure of comparison per branch\n",
    "plt.figure(figsize=(20,6))\n",
    "sns.barplot(data=data1,x=\"Entity\",y=\"Tweet ID\",hue='Sentiment')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Brand\")\n",
    "plt.ylabel(\"Number of tweets\")\n",
    "plt.grid()\n",
    "plt.title(\"Distribution of tweets per Branch and Type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Data Cleaning / Preprocessing for Medeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity Recognition\n",
    "Entities_t = set(twitter_training['Entity'])\n",
    "Entities_v = set(twitter_validation['Entity'])\n",
    "Entities_t = list(Entities_t)\n",
    "Entities_v = list(Entities_t)\n",
    "EntitiesLowered = [item.lower() for item in Entities_t]\n",
    "\n",
    "#See if entiites in both datasets are the same:\n",
    "print(Entities_t)\n",
    "print(len(Entities_t))\n",
    "#print(Entities_v)\n",
    "#print(len(Entities_v))\n",
    "\n",
    "#Creating our entity dictionary:\n",
    "entity_dict = { \n",
    "                \"RedDeadRedemption(RDR)\" : ['rdr', 'red dead redemption', 'red dead'], \n",
    "                \"Microsoft\": ['microsoft'],\n",
    "                \"Xbox(XSeries)\":['xbox', 'series x', 'series s', 'xbox one', 'xseries'], \n",
    "                \"AssassinsCreed\": ['assassinscreed', 'assassins creed'], \n",
    "                \"CallOfDutyBlackopsColdWar\": ['black ops', 'cold war', 'callOfdutyblackopscoldWar'],\n",
    "                \"FIFA\": ['fifa'],\n",
    "                \"TomClancysGhostRecon\": ['ghost recon', 'ghostrecon'],\n",
    "                \"Google\": ['google'],\n",
    "                \"PlayStation(PS)\": ['ps5', 'playstation', 'ps4', 'PS'],\n",
    "                \"Facebook\": ['facebook'],\n",
    "                \"GrandTheftAuto(GTA)\": ['gta', 'grand theft auto'],\n",
    "                \"PlayerUnknownsBattlegrounds(PUBG)\": ['pubg', 'player unknowns battlegrounds', 'PlayerUnknownsBattlegrounds'],\n",
    "                \"Hearthstone\": ['hearthstone'],\n",
    "                \"MaddenNFL\": ['madden'],\n",
    "                \"CallOfDuty\": ['modern warfare', 'call of duty', 'cod'],\n",
    "                \"Fortnite\": ['fortnitegame', 'fortnite'],\n",
    "                \"Verizon\": ['verizon'],\n",
    "                \"Nvidia\": ['nvidia'],\n",
    "                \"Amazon\": ['amazon'],\n",
    "                \"WorldOfCraft\": ['wow', 'world of warcraft'],\n",
    "                \"ApexLegends\": ['apex legends', 'apex', 'apexlegends'],\n",
    "                \"CS-GO\": ['csgo', 'counter strike'],\n",
    "                \"johnson&johnson\": ['johnson&johnson', 'johnson & johnson'],\n",
    "                \"HomeDepot\": ['homedepot', 'home depot'],\n",
    "                \"NBA2K\": ['nba'],\n",
    "                \"Overwatch\": ['overwatch'],\n",
    "                \"LeagueOfLegends\": ['lol', 'league of legends'],\n",
    "                \"Borderlands\": ['borderlands'],\n",
    "                \"TomClancysRainbowSix\": ['rainbow six', 'rainbow six siege', 'rainbowsix'],\n",
    "                \"Dota\": ['dota'],\n",
    "                \"Battlefield\": ['battlefield'],\n",
    "                \"Cyberpunk2077\": ['cyberpunkgame', 'cyberpunk2077', 'cyberpunk'],\n",
    "                \"NintendoSwitch\": ['nintendo switch', 'nintendo'],\n",
    "                \"Windows\": ['windows', 'window']\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By each entry:\n",
    "#Step1: Find Entities in sentence and replace them from Entitylist\n",
    "#Step2: Lower case every- word in every entry\n",
    "\n",
    "numbers_list = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "#comment = \"on Borderlands 1 I will murder all on Xbox the console of microsoft, is like RDR or Assassin's-creed...\"\n",
    "comment = twitter_training[\"Tweet_content\"][292]\n",
    "\n",
    "#create a bag of words: entity_dic = { \"RedDeadRedemption(RDR)\" : [rdr, RDR, Red Dead]}\n",
    "#if word in entity_dic[\"Red Daed Redemption\"]:\n",
    "\n",
    "# Note1: Some ENTITIES are not found like: xbox, PS5, Red Dead Redemption or rdr, this migth cause loss of accuracy. (Check: )\n",
    "# - This doesn't work -\n",
    "#comment=comment.upper()\n",
    "#comment=comment.title()\n",
    "#comment = string.capwords(comment)\n",
    "# Note2: What about diferent games of the same saga like: COD black ops/modern warfare/cold war... what should we do?  (Check: )\n",
    "# Note3: Hardcoding the entities dictionary?\n",
    "print(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = re.sub(r\"[^a-zA-Z4-5 ]\", \"\", comment) #Remove apostrophes, comas, ... \n",
    "print(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = ' '.join([word for word in comment.split() if word not in numbers_list]) #Remove whitespaces\n",
    "comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if string exists in dictionary value\n",
    "\n",
    "example='I love cod black ops'\n",
    "\n",
    "def find_entities_list(text):\n",
    "    text = re.sub(r\"[^a-zA-Z ]\", \"\", text)\n",
    "    text = ' '.join([word for word in text.split()])\n",
    "    text = text.lower()\n",
    "    #print(text)\n",
    "    for entity_names in entity_dict:\n",
    "        for game_names in entity_dict[entity_names]:\n",
    "            if game_names.lower() in text:\n",
    "                #print(game_names)\n",
    "                text = text.replace(game_names, entity_names)\n",
    "                #print(text)\n",
    "    return text\n",
    "\n",
    "print(find_entities_list(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_training[\"Tweet_content\"][24432]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_entities_list(twitter_training[\"Tweet_content\"][71659]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_training['Tweet_content'] = [find_entities_list(word) for word in twitter_training['Tweet_content']]\n",
    "twitter_training['Tweet_content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Tokenization/Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twitter_training['Tweet_content'] = [word.lower() for word in twitter_training['Tweet_content']]\n",
    "#twitter_training['Tweet_content']\n",
    "\n",
    "#Tokenize every entry in the data set\n",
    "tokenized_twitter_training = [word_tokenize(word) for word in twitter_training['Tweet_content']]\n",
    "\n",
    "#Transforming into a single list\n",
    "tokenized_twitter_training = list(map(str, it.chain.from_iterable(tokenized_twitter_training)))\n",
    "#tokenized_twitter_training[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords, punctuation and numbers\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list += list(string.punctuation)\n",
    "stopwords_list += ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "stopwords_list += ['....','...', '..', 'â€™', \"''\", '``', '-',\"([a-zA-Z]+(?:'[a-z]+)?)\"]\n",
    "tokenized_twitter_training_stopped = [word for word in tokenized_twitter_training if word not in stopwords_list]\n",
    "#tokenized_twitter_training_stopped[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Distribution to see the number of times each word is used in each entry.\n",
    "tokenized_twitter_training_freqdist = FreqDist(tokenized_twitter_training_stopped)\n",
    "tokenized_twitter_training_freqdist.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokenized_twitter_training_lemmatized = [lemmatizer.lemmatize(word) for word in tokenized_twitter_training_stopped]\n",
    "tokenized_twitter_training_lemmatized[:10]\n",
    "#lemmatizer.lemmatize('drank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "tokenized_twitter_training_stemmed = [ps.stem(word) for word in tokenized_twitter_training_stopped]\n",
    "tokenized_twitter_training_stemmed[:10]\n",
    "\n",
    "#words = [\"play\", \"playing\", \"played\", \"player\"]\n",
    " \n",
    "#for w in words:\n",
    "#    print(w, \" : \", ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_twitter_training_freqdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "tokenized_twitter_training_stopped_finder = BigramCollocationFinder.from_words(tokenized_twitter_training_stopped)\n",
    "tokenized_twitter_training_stopped_scored = tokenized_twitter_training_stopped_finder.score_ngrams(bigram_measures.raw_freq)\n",
    "# Display the first 10 elements of macbeth_scored\n",
    "tokenized_twitter_training_stopped_scored[:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Train/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "def remove_outlier(df_in, col_name):\n",
    "    q1 = df_in[col_name].quantile(0.25)\n",
    "    q3 = df_in[col_name].quantile(0.75)\n",
    "    iqr = q3-q1 #Interquartile range\n",
    "    fence_low  = q1-1.5*iqr\n",
    "    fence_high = q3+1.5*iqr\n",
    "    df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]\n",
    "    return df_out\n",
    "\n",
    "#remove outliers\n",
    "twitter_training = remove_outlier(twitter_training,'text_lens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenazation and Lemmatization\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens=[]\n",
    "    for token in doc:\n",
    "        if not token.is_stop and not token.is_punct:\n",
    "            filtered_tokens.append(token.lemma_)\n",
    "    return \" \".join(filtered_tokens)\n",
    "    \n",
    "\n",
    "twitter_training['preprocessed_text']=twitter_training['Tweet_content'].apply(lambda x: preprocess(x))\n",
    "twitter_validation['preprocessed_text']=twitter_validation['Tweet_content'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=twitter_training['Tweet_content'].apply(lambda x: preprocess(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train ,X_test , y_train, y_test = train_test_split(\n",
    "    twitter_training[['preprocessed_text']],\n",
    "    twitter_training[['Sentiment']],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
